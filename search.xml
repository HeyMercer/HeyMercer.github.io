<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Plan C</title>
    <url>/2021/04/03/Plan-C/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="就这？" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="6fd04423a91c695a2c840e4850a7566224af27cb54453cfcff9c29f28870485b">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">猜对算我输</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Classified</category>
      </categories>
      <tags>
        <tag>Classified</tag>
      </tags>
  </entry>
  <entry>
    <title>高考小记</title>
    <url>/2021/06/05/%E9%AB%98%E8%80%83%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>  高考前的一个月，或者说整个高三下，我的印象都非常清晰。</p>
<p>  高三下我过得非常不好。就好像大家都在泳池的赛道里齐头并进的划水、蹬腿，自己却突然猛呛了一口水，开始挣扎，慢慢沉落。然而人们都忙着冲向终点，没有发觉这不起眼的小插曲、小浪花。</p>
<p>  不过回想起自己的朋友，当初像我一样沉在水底的人也不少吧。</p>
<br>
<p>  高考的前两周，每天清晨走在去吃早饭的小路上，都感觉前面有一行大大的“离高考还有xxx天”浮在空中，然后我“刷”的穿过这行字，带着奇异的心情开始一天的旅程。听课，做题，复习，总结… 高考前的每一天都很“满”，能做完作业已经是我的极限了，没有空去买额外的题集做练习。那时杨老头长期给我们灌输心灵鸡汤：“只要努力就一定——”</p>
<p>  高考的前一晚上，教室里的人各自怀揣着不同的心理，或安静复习，或和往常一样的扎堆聊天。我自己一个人在教室旁的杂物间，靠着栏杆里吃香蕉，试着放空。老实说当时的我并不紧张，但是有压力，毕竟可能是人生最重要的一场考试。</p>
<p>  后来陈彦廷也进了杂物间。于是自然而然地开始讨论起明天的考试，讨论起高考，甚至想想将来。陈彦廷说，这都是命，考的好考不好，都是命里注定的。我不置可否。我觉得人活着就像在一张图的某个结点上，有着无数个分支，无数条路，从当下前往不同的结点。人有概率的选择其中某一个分支，走向未来。即使不能百分百地选择我们最想走的那条路，但我认为人当下要做的，就是把最想走的那条路的概率，让他无限趋近于1。我们为了高考不断地刷题，考试，的确也无法100%的考好，但我们可以追求那99.9999999%. 即使最后那0.0000001%发生了，对我来说，我也不后悔。</p>
<br>
<p>  早上考完语文，整体也还算平滑。考试开始的时候感觉一点也不紧张，最后1分钟检查卷子的时候倒是心跳越来越快，怕机读卡或者身份信息哪里填错了，反复的检查，直到考试铃一响，虚出一身冷汗。</p>
<p>  坐在我前面的小个子女生，长得特别像我的生物老师Lily。所以我暗暗称她为小Lily。旁边的考生考前一般在发呆或者玩手指，考完就像解放了一样迅速离开。只有小Lily考前正襟危坐，考后慢慢地收拾文具准备离开。考完语文，我四下观察了小Lily以及考场里其他同学的反应，大多都还算放松。看来大家都觉得不是很难。我自己该答的都答了，作文写的很平庸，只能听天由命了。</p>
<p>  下午的数学考的很简单，反而让我起了很重的疑心，前面的选择题做得很慢，导致最后几何的最后一题没有做完，实乃大失误。不过好在估摸了一下，也不会和别人拉开大差距，想想也就过去了。</p>
<p>  晚上也是在正常的复习中度过。中途大家跑出教室到操场去散步（还是去摘桃？），我和少数几个同学留下来继续复习。当时觉得自己没什么好散心的，不如安心呆在教室里自习。</p>
<p>  第二天的理综考的极差，化学有点小蒙，到了最后一秒还在纠结一个生物选择题，改了选项。下午的英语倒是四平八稳，觉得没什么大问题。整体高考下来，觉得卷子还是不算难的，可能就是比谁失误失误的少了吧。</p>
<p>  走在去校车的路上，准备回学校，突然在另外一辆别的学校的校车上瞥见了小Lily。那辆校车上的人们看起来都在说着这两天的考试，或者过会儿准备去哪玩，热闹地扎堆讨论着，时不时传来男同学洪亮地声音。小Lily静静地坐在自己的座位上，头靠着窗户，看着外面的柏油路。</p>
<p>  我看见她叹了一口气。</p>
<br>
<p>  晚上回到家，临睡前刷了会儿B站。突然看到化学老师的QQ群里有人晒了化学的答案，一下子就清醒了。马上把答案对了一下，感觉很慌，错的不少。后面又给了各科答案的链接，于是心想，化学都对了，干脆就对完算了。于是一口气把答案对了遍。整体来说还算不错，不过那道最后临时改的生物选择果然从正确选项改成了错误选项，深为痛心。</p>
<p>  对完了就睡不着了，开始估分，680？感觉不可能那么高。不过反正就是翻来覆去的睡不着，要么耍手机，要么又开始重新估分。于是乎就到了凌晨四点。最后把欠别人的一张生日贺卡补写了，终于倒在床上，慢慢等待睡梦的来临。</p>
<p>  之后英语口语，散伙饭，毕业典礼，晚上和好友去吃饭唱K玩毕业真心话，其实每件事都可以拣来讲讲。很多人在高考之后，都彻底甩开了学习，做一些专属于青春的仪式，要么打算用自己的方式和高中告别，要么打算做一些自己一直想要做的事。而我要么听说，要么目睹了这种种的“仪式”，或者说是高考后终于冒出尖尖的心思。我也有我自己的想法，但我在很久之后才算真正完成了。不过既然完成了，也不算后悔。</p>
<br>
<p>  出分的那天自己处于半紧张的状态。还没到查分的时候，就开始陆陆续续有各种信息像雨后春笋一样冒出来了，搞得我很烦。另外一边的陈昭是真紧张着急，我记得她那天下午和我聊天就一直处于一种焦虑的状态，担心成绩也担心未来。</p>
<p>  很多人的分都出来了，都挺高，吓了我一跳，然而我老是登不上查分网，网页总是崩溃。</p>
<p>  好不容易登上去了，于是我马上让父母别查，也让他们别找别人帮查，我自己一定一定要第一眼看我自己的成绩。于是跑到黑暗的卧室里开始输入身份信息。成绩页面出来的那一刻我瞬间用手盖住，那个时候心跳动的很厉害。于是慢慢把手往下移，看到语文125后开始有点小欣喜，然后就忍耐不住直接把手移开，看到了自己总分:691。</p>
<p>  我突然想起来《你好旧时光》里写的余周周的查分的场景，于是像她一样，装作一脸沮丧的走出卧室回到客厅，妈妈看到我的表情感觉不对，担心的说了句“没考好啊？”。于是我把手机给她看，自己头低下去，开始暗暗窃喜。</p>
<p>  “这个是多少… 天哪！”</p>
<br>
<p>  那一晚上就是在亲人之间奔走相告，我只和我以前的关系很好的老师打了电话，报了喜讯。那天看分数的时候还觉得清北有望，复交保底。第二天早上看分段榜，四川有100多个700分以上的——原来是我想多了。</p>
<p>  后来提前批没报上上交工科班，也不想去读自然和核物理，于是最后去了第一志愿的浙大工科。当时得知自己被录取的时候，我还在和同学在日本玩。知道了录取之后，抛下了去秋叶原shopping的同学，一个人安静的坐在东京的Palette Town，看着窗外的波光粼粼。</p>
<p>  一切也算尘埃落定，过往云烟成雨。</p>
<br>
<p>  几家欢喜几家愁是最适合拿来形容高考的。朋友中有厉害的却考得不好，有平常一般的却超常发挥的。杨老头当初说“只要努力就一定——”，像是失灵了。</p>
<p>  但我想，也不算。</p>
<p>  我们19年高考的这一代在那个暑假，慢慢和过去的自己，过去的中学时光道别，踏上了崭新的征程，遇见新的故事和新的难题。现在回首当时如此重要的高考，其实也就是人生当中的一个分水岭，但并不是唯一的。厉害的同学到了普通的大学仍然散发着光芒，而那些去了Top院校的同学却也开始挣扎在保研和内卷的压力之中。身边的东西都在慢慢改变，但是没有改变的是，我们仍然地重复着，做99.99999999%的趋近，走自己最想要的那条路。无论过往多少次0.000000001%的发生，我们都不依不挠地努力着。</p>
<p>  感谢过往当时那个缺课一年也不放弃自己的自己，感谢那个在高三下“溺水”却也拼命挣扎着要冲上对岸的自己。站在人生的这张充满概率的图上，我们不占有“只要努力就一定”的天赋，却拥有“只有努力才方可”的能力，不断趋近着，那个理想的”1“。</p>]]></content>
      <categories>
        <category>Recall</category>
        <category>Senior</category>
      </categories>
      <tags>
        <tag>Recall</tag>
        <tag>Gaokao</tag>
        <tag>Nostalgia</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Sec 4~5</title>
    <url>/2021/05/31/CS224W-Sec-4-5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1>CS224W Sec 4~5</h1>
<h2 id="Section-4"><a class="header-anchor" href="#Section-4">¶</a>Section 4</h2>
<h3 id="4-1-Pipeline"><a class="header-anchor" href="#4-1-Pipeline">¶</a>4.1 Pipeline</h3>
<h4 id="PageRank"><a class="header-anchor" href="#PageRank">¶</a>PageRank</h4>
<ul>
<li>
<p>Idea: Use Link Analysis approaches to compute the importance of nodes</p>
<ul>
<li>Links as votes</li>
</ul>
</li>
<li>
<p>PageRank</p>
<ul>
<li>
<p>Flow Model</p>
<p>Each link’s vote is proportion to the importance of its source page.</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/flow.png" alt></p>
<ul>
<li>A page is important if it is pointed to by other important pages</li>
</ul>
</li>
<li>
<p>FlowModel: Use matrix to solve the equation instead of Gauss</p>
</li>
</ul>
</li>
<li>
<p>Matrix Formulation</p>
<ul>
<li>
<p>Stochastic adjacency matrix and rank vector</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/matrix.png" alt></p>
</li>
<li>
<p>Connection to Random Walk</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/p.png" alt></p>
<p>Then <code>p(t+1) = M * p(t)</code></p>
</li>
<li>
<p>Stationary Distribution: <code>p(t) = M * p(t)</code></p>
</li>
<li>
<p>If r is the limit of M M M M M u, then r satisfies the flow equation 1 * r = M * r; So r is the principal eigenvector.</p>
<ul>
<li>Solution: Power iteration</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-1-Summary"><a class="header-anchor" href="#4-1-Summary">¶</a>4.1 Summary</h3>
<ul>
<li>Measures importance of nodes in a graph through link structure</li>
<li>Models a random web surfer using the <strong>stochastic adjacency matrix M</strong></li>
<li>PageRank solves r = Mr where r can views as both the stationary distribution of a random work and the principle eigenvector of M.</li>
</ul>
<p>​</p>
<h3 id="4-2-Pipeline"><a class="header-anchor" href="#4-2-Pipeline">¶</a>4.2 Pipeline</h3>
<h4 id="PageRank-How-to-solve"><a class="header-anchor" href="#PageRank-How-to-solve">¶</a>PageRank: How to solve</h4>
<ul>
<li>
<p>How to solve the PageRank equation</p>
<ul>
<li>
<p>Assign each node an initial page rank</p>
</li>
<li>
<p>Repeat <code>p(t+1)=p(t) * M</code> until convergence</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/power.png" alt></p>
</li>
</ul>
</li>
<li>
<p>Question</p>
<ul>
<li>Does this converge</li>
<li>Does it converge to what we want?</li>
<li>Are results reasonable?</li>
</ul>
</li>
<li>
<p>Two problems in converge</p>
<ul>
<li>Spider Trap
<ul>
<li>Importance flows to one node</li>
</ul>
</li>
<li>Dead end
<ul>
<li>Some nodes has no out links, and the importance continuously reduces</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Solutions:</p>
<ul>
<li>
<p>Spider Trap: At each time step, the random surfer has two options</p>
<ul>
<li>With prob. β, follow a link at random</li>
<li>with prob. 1 - β, jump to a random page</li>
</ul>
</li>
<li>
<p>Dead ends: Follow random teleport links with total prob. 1.0 from dead-ends</p>
</li>
<li>
<p>Fixed PageRank Equation:</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/equation.png" alt></p>
</li>
<li>
<p>Example:</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/CS224W-Sec-4-5%5Cexample.png" alt></p>
</li>
</ul>
</li>
</ul>
<h3 id="4-2-Summary"><a class="header-anchor" href="#4-2-Summary">¶</a>4.2 Summary</h3>
<ul>
<li>PageRank solves for r = Gr and can be efficiently computed by power iteration</li>
<li>Adding random uniform teleportation</li>
</ul>
<h3 id="4-3-Pipeline"><a class="header-anchor" href="#4-3-Pipeline">¶</a>4.3 Pipeline</h3>
<h4 id="Random-Walk-with-Restarts"><a class="header-anchor" href="#Random-Walk-with-Restarts">¶</a>Random Walk with Restarts</h4>
<ul>
<li>
<p>Goal: Proximity (Related Recommendation)</p>
<ul>
<li>How to define a metric?(Shortest path, neighbors…)</li>
</ul>
</li>
<li>
<p>Personalized PageRank:</p>
<ul>
<li>Ranks proximity of nodes to the teleport nodes S</li>
<li>Q: What is most related item to <strong>Item Q</strong></li>
<li>Random Walks with Restarts:
<ul>
<li>Teleport back to the starting node: S ={Q}</li>
<li>Tips: S = V previously(In 4.2 random walks)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Pixie Random Walk Algorithm(On bipartite)</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/Palgorithm.png" alt></p>
<ul>
<li>
<p>Higher visit counts means:</p>
<ul>
<li>More paths</li>
<li>More common neighbors</li>
</ul>
</li>
<li>
<p>You can <strong>also use power iteration</strong> to achieve</p>
</li>
</ul>
<h3 id="4-3-Summary"><a class="header-anchor" href="#4-3-Summary">¶</a>4.3 Summary</h3>
</li>
<li>
<p>PageRank:</p>
<ul>
<li>Teleports to any node</li>
<li>Nodes can have the same probability of the surfer landing</li>
</ul>
</li>
<li>
<p>Topic Specific PageRank aka Personalized PageRank</p>
<ul>
<li>Teleports to a specific set of nodes</li>
<li>Have different probabilities</li>
</ul>
</li>
<li>
<p>Random Walk with Restarts</p>
<ul>
<li>Teleports to the same node</li>
</ul>
</li>
</ul>
<h3 id="4-4-Pipeline"><a class="header-anchor" href="#4-4-Pipeline">¶</a>4.4 Pipeline</h3>
<h4 id="Embeddings-Matrix-Factorization"><a class="header-anchor" href="#Embeddings-Matrix-Factorization">¶</a>Embeddings &amp; Matrix Factorization</h4>
<ul>
<li>
<p>Connection to Matrix Factorization(<strong>Important： Z<sup>T</sup>Z=A</strong>)</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/CS224W-Sec-4-5%5Cconnect.png" alt></p>
</li>
<li>
<p>Not Impossible: The embedding dimension d &lt;&lt; n</p>
<ul>
<li>But we can learn <strong>Z</strong> approximately
<ul>
<li>min ||A - Z<sup>T</sup>Z||<sub>2</sub></li>
<li>Based on connectivity</li>
</ul>
</li>
</ul>
</li>
<li>
<p>DeepWalk and node2vec have a more complex node similarity definition based on <strong>random walks</strong></p>
</li>
<li>
<p>Limitation of node embeddings via matrix factorization and random walks</p>
<ul>
<li>Can’t obtain embeddings for nodes not in the training set
<ul>
<li>Need to recompute all node embeddings once added</li>
</ul>
</li>
<li>Can’t capture structural similarities
<ul>
<li>If two node u, v have similarities structurally but there is a long distance between them, it’s unlikely that a random walk will reach v from u.</li>
</ul>
</li>
<li>Can’t utilize node, edge and graph features</li>
</ul>
</li>
<li>
<p>Solution: Deep Representation Learning and GNN</p>
</li>
</ul>
<p><strong>Viewing graphs as matrices play a key role in all above algorithms</strong></p>
<h2 id="Words"><a class="header-anchor" href="#Words">¶</a>Words</h2>
<p>navigational: 导航的</p>
<p>citation: 引用</p>
<p>encyclopedia:[ɪnˌsaɪkləˈpiːdiə]: 百科全书</p>
<p>trustworthy: 值得信任的，可靠的</p>
<p>stochastic: 随机的</p>
<p>surfer: 漫游者</p>
<p>hypothetical: 假定的</p>
<p>stabilize: 稳定</p>
<p>eigenvector: 特征向量</p>
<p>converge: 集中，相似，相同</p>
<p>teleport: 传送</p>
<p>leak out: 露出，走风，泄露</p>
<p>metric: 标准</p>
<p>evenly: 平滑的，有规律的，均匀的</p>
<p>limiting distribution: 极限分布(随机变量列的极限的概率分布)</p>
<p>matrix factorization: 矩阵分解</p>
<p>discrepancy: 差异</p>
<p>albeit:[ˌɔːlˈbiːɪt]:虽然，尽管</p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Lab1</title>
    <url>/2021/05/30/CS224W-Lab1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1>CS 224W Lab1</h1>
<h2 id="Question-1"><a class="header-anchor" href="#Question-1">¶</a>Question 1</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">avg_degree = <span class="built_in">round</span>(num_edges * <span class="number">2</span> / num_nodes)</span><br></pre></td></tr></table></figure>
<p><strong>Note</strong>: Use Function <code>round()</code> to round the result.</p>
<h2 id="Question-2"><a class="header-anchor" href="#Question-2">¶</a>Question 2</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">avg_cluster_coef = <span class="built_in">round</span>(nx.average_clustering(G), <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>About clustering coefficient:</p>
<p><img src="/2021/05/30/CS224W-Lab1/image-20210528172335527.png" alt="image-20210528172335527"></p>
<p>Call the function of NetworkX to calculate the answer.</p>
<h2 id="Question-3"><a class="header-anchor" href="#Question-3">¶</a>Question 3</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ni <span class="keyword">in</span> nx.neighbors(G, node_id):</span><br><span class="line">  r1 += beta * r0 / G.degree[ni]</span><br><span class="line">r1 += (<span class="number">1</span> - beta) * r0</span><br><span class="line">r1 = <span class="built_in">round</span>(r1, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>Pay attention to that: we only need to calculate <code>PageRank</code> of <code>node 0</code> after one iteration. So we could use r<sub>0</sub> to replace r<sub>i</sub> .</p>
<p>More specific algorithm:</p>
<p><img src="/2021/05/30/CS224W-Lab1/image-20210528210137487.png" alt="image-20210528210137487"></p>
<h2 id="Question-4"><a class="header-anchor" href="#Question-4">¶</a>Question 4</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sigma_shortd = nx.shortest_path_length(G, source = node)</span><br><span class="line"><span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> ni <span class="keyword">in</span> <span class="built_in">range</span> (G.number_of_nodes()):</span><br><span class="line">  <span class="keyword">if</span> (ni != node):</span><br><span class="line">    <span class="built_in">sum</span> += sigma_shortd[ni]</span><br><span class="line">closeness = <span class="built_in">round</span>(<span class="number">1</span> / <span class="built_in">sum</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">closeness = nx.closeness_centrality(G, u = node) / (G.number_of_nodes() - <span class="number">1</span>)</span><br><span class="line"><span class="comment">#Divide n-1 (n=nodes that u could reach). See the docs</span></span><br></pre></td></tr></table></figure>
<p>The docs for function: <a href="https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path_length.html">shortest_path_length</a></p>
<p>The docs for function: <a href="https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.closeness_centrality.html?highlight=closenes#networkx.algorithms.centrality.closeness_centrality">closeness_centrality</a></p>
<h2 id="Question-5"><a class="header-anchor" href="#Question-5">¶</a>Question 5</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_list = <span class="built_in">list</span>(G.edges())</span><br></pre></td></tr></table></figure>
<p>The return type of G.edges() is EdgeView. Usually iterates as tuple(u, v) or (u, v, d). (default = false , 2 tuple). Can also be used for attribute look up as <code>edges[u, v]['foo']</code>.</p>
<p>Using append to add element is also feasible.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_index=torch.LongTensor(edge_list).t()</span><br></pre></td></tr></table></figure>
<p>t() is to transpose.</p>
<h2 id="Question-6"><a class="header-anchor" href="#Question-6">¶</a>Question 6</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">neg_edge_list = <span class="built_in">list</span>(nx.non_edges(G))</span><br><span class="line">neg_edge_list = random.sample(neg_edge_list, num_neg_samples)</span><br></pre></td></tr></table></figure>
<p>Use the function in NetworkX to get the edges that not exist and random.sample to sample the neg_edge_list.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span>(<span class="params">cedge</span>):</span>    <span class="keyword">return</span> cedge <span class="keyword">in</span> neg_edge_list <span class="keyword">or</span> (cedge[<span class="number">1</span>], cedge[<span class="number">0</span>]) <span class="keyword">in</span> neg_edge_listprint(check(edge_1))print(check(edge_2))print(check(edge_3))print(check(edge_4))print(check(edge_5))</span><br></pre></td></tr></table></figure>
<p>It’s undirected, so (u, v) and (v, u) is identical.</p>
<h2 id="Question-7"><a class="header-anchor" href="#Question-7">¶</a>Question 7</h2>
<p>Note: <code>Sklearn</code> is a library for many aspects of machine learning.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emb = nn.Embedding(num_node, embedding_dim)  emb.weight.data = torch.rand(num_node, embedding_dim)</span><br></pre></td></tr></table></figure>
<p><code>torch.rand()</code> include a group of random numbers under [0, 1) uniform distribution.</p>
<p><code>pca = PCA(n_components= x)</code> The dimension will be reduced to x.</p>
<p>Use <code>plt.scatter</code> to draw scatter diagram.</p>
<p>numpy()  is to transform the data type.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accu = <span class="built_in">round</span>(((pred &gt; <span class="number">0.5</span>) == label).<span class="built_in">sum</span>().item() / (pred.shape[<span class="number">0</span>]), <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>I learned this way from github(By <strong>Polaris</strong>): pred == label will not return a boolean value but a tensor. After sum(), it will become a tensor with only one value. Then use item() to extract this value as a scalar.</p>
<ul>
<li>epoch: All the data in the sample are computed for once is called a epoch</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.zero_grad()  </span><br><span class="line">train_node_emb=emb(train_edge)</span><br><span class="line">dot_product_result=train_node_emb[<span class="number">0</span>].mul(train_node_emb[<span class="number">1</span>])</span><br><span class="line">dot_product_result=torch.<span class="built_in">sum</span>(dot_product_result,<span class="number">1</span>) </span><br><span class="line">sigmoid_result=sigmoid(dot_product_result) </span><br><span class="line">loss_result=loss_fn(sigmoid_result,train_label)  </span><br><span class="line">loss_result.backward()</span><br><span class="line">optimizer.step()  </span><br><span class="line">print(loss_result) </span><br><span class="line">print(accuracy(sigmoid_result,train_label))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Lab0</title>
    <url>/2021/05/30/CS224W-Lab0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1>CS224 W Lab0</h1>
<ul>
<li>
<p>Introduce some basic concepts of graph mining and Graph Neural Networks.</p>
</li>
<li>
<p>Two Packages:</p>
<ul>
<li>
<p>NetworkX</p>
<p>For Graph: <a href="https://networkx.org/documentation/stable/reference/classes/index.html">NetworkX graph types</a></p>
<p><img src="/2021/05/30/CS224W-Lab0/image-20210527212511812.png" alt="image-20210527212511812"></p>
</li>
<li>
<p>PyTorch Geometric</p>
</li>
</ul>
</li>
</ul>
<h2 id="NetworkX-Tutorial"><a class="header-anchor" href="#NetworkX-Tutorial">¶</a>NetworkX Tutorial</h2>
<p>NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs.</p>
<ul>
<li>
<p><code>G = nx.Graph</code></p>
<p>Create a new empty undirected graph. It could also be:</p>
<p><code>G = nx.DiGraph</code> etc.</p>
</li>
<li>
<p><code>G.is_directed</code></p>
<p>To judge whether it’s directed graph, return the boolean value.</p>
</li>
<li>
<p><code>G.graph[&quot;name&quot;] = &quot;Bar&quot;</code></p>
<p>Add attribute pair.</p>
</li>
<li>
<p><code>G.add_node(node_for_adding, **attr)</code></p>
<p>Add the node and update node attributes(could follow more than 1).</p>
<ul>
<li>New way for <code>print</code>: <code>print.format</code> by <code>&#123;&#125;</code></li>
</ul>
</li>
<li>
<p><code>G.add_nodes_from()</code></p>
<p>A more flexible way to add nodes. (Add list or something)</p>
<p><img src="/2021/05/30/CS224W-Lab0/image-20210527214842221.png" alt="image-20210527214842221"></p>
<ul>
<li>
<p>About <code>(data=True)</code></p>
<p>It could return the attributes information of node when printing.<strong>(Need deeper understanding here)</strong></p>
<p><img src="/2021/05/30/CS224W-Lab0/image-20210527215110377.png" alt="image-20210527215110377"></p>
</li>
</ul>
</li>
<li>
<p>About Edge</p>
<p>Change the <code>node_for_adding</code> to a pair of nodes.</p>
</li>
<li>
<p>Visualization</p>
<p><code>nx.draw(G, with_labels = True)</code></p>
</li>
<li>
<p>Other Functions</p>
<ul>
<li><code>G.degree[node_id]</code>: Get the degree of node_id</li>
<li><code>G.neighbors(node_id)</code>: Get the set of the neighbors of node_id</li>
<li>More Information: <a href="https://networkx.org/documentation/stable/">Click Here</a></li>
</ul>
</li>
</ul>
<h2 id="PyTorch-Geometric-Tutorial"><a class="header-anchor" href="#PyTorch-Geometric-Tutorial">¶</a>PyTorch Geometric Tutorial</h2>
<p>PyTorch Geometric (PyG) is an extension library for PyTorch. It provides useful primitives to develop Graph Deep Learning models, including various graph neural network layers and a large number of benchmark datasets.</p>
<p><strong>Wait for filling…</strong></p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Sec.1~3</title>
    <url>/2021/05/30/CS224W-Sec-1-3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1>CS 224W Sec1~3</h1>
<h2 id="Section-1"><a class="header-anchor" href="#Section-1">¶</a>Section 1</h2>
<h3 id="Pipeline"><a class="header-anchor" href="#Pipeline">¶</a>Pipeline</h3>
<p>It gives a brief introduction to the course and some basic concepts of Graph.</p>
<h3 id="Summary"><a class="header-anchor" href="#Summary">¶</a>Summary</h3>
<ul>
<li>Machine learning with Graphs
<ul>
<li>Applications and use cases</li>
</ul>
</li>
<li>Different types of tasks
<ul>
<li>Node level</li>
<li>Edge level</li>
<li>Graph level</li>
</ul>
</li>
<li>Choice of a graph representation
<ul>
<li>Directed, undirected, bipartite, weighted, self edges, multigraph</li>
<li>Adjacency matrix, edge list, adjacency list</li>
<li>Connectivity(Scc,etc.)</li>
</ul>
</li>
</ul>
<h2 id="Section-2"><a class="header-anchor" href="#Section-2">¶</a>Section 2</h2>
<p>Waiting for summary.</p>
<h2 id="Section-3"><a class="header-anchor" href="#Section-3">¶</a>Section 3</h2>
<h3 id="3-1-Pipeline"><a class="header-anchor" href="#3-1-Pipeline">¶</a>3.1 Pipeline</h3>
<p>Use Representation Learning to replace Feature Engineering: <em><strong>Auto</strong></em> replace <em><strong>Manual</strong></em>.</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529192748944.png" alt="image-20210529192748944"></p>
<p>The vector that is extracted from Graph is called Feature representation, or <em><strong>Embedding</strong></em>.</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529193143639.png" alt="image-20210529193143639"></p>
<p><strong>Goal: Map nodes into an embedding space, to approximate</strong></p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529193411637.png" alt="image-20210529193411637"></p>
<p><strong>Steps:</strong></p>
<ol>
<li>Encoder maps from nodes to embeddings</li>
<li>Define a node similarity function</li>
<li>Decoder maps from embeddings to the similarity score</li>
<li>Optimize the parameters of the encoder so that: The similarity of the original network approximate the similarity of embedding**(Usually as dot(Z<sub>v</sub><sup>T</sup>Z<sub>u</sub>))**</li>
</ol>
<p>Many method: <strong>DeepWalk</strong>, node2vec</p>
<p>Node similarity:</p>
<ul>
<li>Are linked?</li>
<li>Share neighbors?</li>
<li>Have similar &quot;structural roles &quot;?(like graphlet, kernel)</li>
</ul>
<p>Use <strong>random walks</strong> to learn node similarity definition.</p>
<p>These embeddings are <strong>task independent</strong>, without utilizing node labels and features</p>
<h3 id="3-1-Summary"><a class="header-anchor" href="#3-1-Summary">¶</a>3.1 Summary</h3>
<p>Encoder + Decoder Framework</p>
<ul>
<li>
<p>Shallow encoder: embedding lookup</p>
</li>
<li>
<p>Parameters to optimize: Z which contains node embeddings z<sub>u</sub> for all nodes u ∈ V</p>
</li>
<li>
<p>Deep encoders(GNNs) will be covered in Lecture 6</p>
</li>
<li>
<p>Decoder : Based on node similarity</p>
</li>
<li>
<p>Objective: maximize z<sub>v</sub><sup>T</sup>z<sub>u</sub> for node pairs (<em>u, v</em>) that are similar</p>
</li>
</ul>
<h3 id="3-2-Pipeline"><a class="header-anchor" href="#3-2-Pipeline">¶</a>3.2 Pipeline</h3>
<ul>
<li>
<p>Notation - Tired to type</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529203256631.png" alt></p>
</li>
<li>
<p>Random-Walk</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529204527933.png" alt></p>
<ul>
<li>
<p>Definition: A walk at random</p>
</li>
<li>
<p>Random-Walk Embedding:</p>
<p>z<sub>u</sub><sup>T</sup>z<sub>v</sub> ≈ probability that u and v co-occur on a random walk over the graph</p>
</li>
<li>
<p>Expressivity: Incorporates both local and higher-order neighborhood information</p>
<ul>
<li>Idea: If random walk starting from node u visits v with higher probability, u and v are similar(higher-order multi-hop information)</li>
</ul>
</li>
<li>
<p>Efficiency: Only need to consider pairs that co-occur on random walks</p>
</li>
</ul>
</li>
<li>
<p>Feature Learning as Optimization</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529204955162.png" alt></p>
</li>
<li>
<p>Steps:</p>
<ol>
<li>Run <strong>short fixed-length random walks</strong> (By strategy R)</li>
<li>For each node u collect N<sub>R</sub>(u) as <u>multiset</u> (can have repeated elements since nodes can be visited multiple times on random walks)</li>
<li>Optimize the function above -&gt; Maximum likelihood objective</li>
</ol>
</li>
<li>
<p>Use softmax to parameterize P(v | z<sub>u</sub>)</p>
<ul>
<li><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529211134940.png" alt="image-20210529211134940"></li>
</ul>
</li>
</ul>
<p><strong>Basic Function</strong>:</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529211453112.png" alt="image-20210529211453112"></p>
<p>Improvement:</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529212644608.png" alt></p>
<p>In practice, k = 5 ~ 20.</p>
<ul>
<li>Higher k gives more robust estimates</li>
<li>Higher k corresponds to higher bias on negative events</li>
</ul>
<p>Optimize the final function by: <strong>Stochastic Gradient Descent</strong></p>
<p>How to random walk?</p>
<ul>
<li>
<p>Key observation: Flexible notion of network neighborhood N<sub>R</sub>(u) of node u leads to rich node embeddings</p>
</li>
<li>
<p>node2vec: Biased Walks</p>
<ul>
<li>Idea: Use flexible, biased random walks that can trade off between <strong>local</strong> and <strong>global</strong> view of the network.</li>
<li>Two classic strategies: BFS and DFS</li>
</ul>
</li>
<li>
<p>Define two parameters: <em>p</em> and <em>q</em></p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529215517966.png" alt="image-20210529215517966"></p>
</li>
</ul>
<h3 id="3-2-Summary"><a class="header-anchor" href="#3-2-Summary">¶</a>3.2 Summary</h3>
<ul>
<li>Core idea: Embed nodes so that distances in embedding space reflect node similarities in the originial network.</li>
<li>Notion of node similarity
<ul>
<li>Naive: similar if 2 nodes are connected</li>
<li>Neighborhood overlap(Section 2)</li>
<li>Random walk approaches(Section 3)</li>
</ul>
</li>
<li>Method
<ul>
<li>No one method wins in all cases
<ul>
<li>node2vec performs better on node classification</li>
</ul>
</li>
<li>Must choose definition of node similarity that matches your application</li>
</ul>
</li>
</ul>
<h3 id="3-3-Pipeline"><a class="header-anchor" href="#3-3-Pipeline">¶</a>3.3 Pipeline</h3>
<ul>
<li>
<p>Goal: Graph -&gt; Embedding</p>
<ul>
<li>
<p>Idea 1: Sum(or average) the node embeddings in G</p>
<ul>
<li>Used in 2016 to classify molecules based on graph structure(successful)</li>
</ul>
</li>
<li>
<p>Idea 2: Add virtual node to connect the graph and run embedding technique(like deep walk and so on), and use the embedding of this virtual node</p>
</li>
<li>
<p>Idea 3: Anonymous Walk Embedding(Record by index)</p>
<ul>
<li>Agnostic to the identity of nodes visited</li>
</ul>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210530143442456.png" alt="image-20210530143442456"></p>
<ul>
<li>
<p>Represent the graph as a probability distribution over these walks(Grows exponentially)</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210530143934648.png" alt="image-20210530143934648"></p>
</li>
<li>
<p>The number of walks we need: see slide P58</p>
</li>
<li>
<p>How to? : See slide(Quite complex to note)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-3-Summary"><a class="header-anchor" href="#3-3-Summary">¶</a>3.3 Summary</h3>
<ul>
<li>Approach 1: Aggregate node embeddings</li>
<li>Approach 2: Create super-node or virtual node</li>
<li>Approach 3: Anonymous Walk Embedding
<ul>
<li>Sample the walks and represent the graph as fraction of probability</li>
<li>Embed anonymous walks</li>
</ul>
</li>
</ul>
<h2 id="End"><a class="header-anchor" href="#End">¶</a>End</h2>
<p>Quick view of machine learning (From website)</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210530202849172.png" alt="image-20210530202849172"></p>
<h2 id="Words"><a class="header-anchor" href="#Words">¶</a>Words</h2>
<p>alleviate: [əˈliːvieɪt] : 缓解，减轻</p>
<p>shallow : 浅的</p>
<p>supervise: 监督</p>
<p>co-occur:  共现</p>
<p>stochastic: [stə’kæstɪk] : 随机的，有可能性的</p>
<p>incorporate: 将…包括在内，包含，吸收，使并入</p>
<p>expressivity: 表现度</p>
<p>intuition: 直觉 (a way of ML)</p>
<p>likelihood: 可能， 可能性</p>
<p>objective: 目标</p>
<p>culprit: [ˈkʌlprɪt] : 罪魁祸首</p>
<p>uniform: 一致的，统一的</p>
<p>biased: 偏向性的</p>
<p>proportional: 成比例的，相称的</p>
<p>convergence：趋同，融合</p>
<p>derivative: n.派生物 adj. 缺乏独创性的</p>
<p>tune: 调整</p>
<p>constrained: 不自然的，过于受约束的</p>
<p>micro(macro)-view: 微观（宏观）视图</p>
<p>mimic: 模仿</p>
<p>overlap: 重叠</p>
<p>agnostic: 不可知论的</p>
<p>cluster: 聚集</p>
<p>concatenate: 连锁</p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>四月小结</title>
    <url>/2021/05/17/%E5%9B%9B%E6%9C%88%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1373228032&auto=1&height=66"></iframe>
<h2 id="课程"><a class="header-anchor" href="#课程">¶</a>课程</h2>
<p>  四月是春学期的最后一个月。</p>
<p>  法学通论是短学期课程，期末作业要交两篇论文。第一篇是关于一个胎盘归属问题的答复，第二篇是关于许霆案的“极限正义”讨论问题。第一篇是个民法的问题，老师给了一些提示，不过对于我这种门外汉来说还是一窍不通。上课的确有很多有趣的案例分析，不过还是比较浅显，感觉不得要义，也怕写论文的时候显得像个法盲。于是把案例发给了现在在上海学法律的高中同桌zhy，和他qq讨论了一番，再打了一通电话，总算是把案件的论点和逻辑整理的比较清楚了。</p>
<p>  第二篇的“极限正义”就是真的蒙圈了，从来没听过这个概念。第二篇是讲刑法的老师出的题，于是就去问zhy刑法里面有没有极限正义的概念。结果他说他也没学过。好在想起了刑法课刚开始的时候，讲刑法的老师提过他最近出的一本书就叫《极限正义》。赶紧在微店上找到了这本书并下单，收到了打开一翻：果然有关于许霆案的分析！也算是偷看了老师自己写的答案了。仔细读懂了极限正义的定义和老师书里的分析，按照自己的思路写了论文，把第二篇也交了上去。</p>
<p>  最后出分只有92，本以为偷看了”答案“也许可以满绩？不过对于我这种外行来说也算不错了，况且这门课的给分好像就不太好。</p>
<p>  专业课还是照常的走。计组的实验真是和上学期的数逻一样恶心，上学期数逻sqs班的实验简直就是地狱折磨，没想到这学期硬件课逃到了zmm门下，实验居然还是跟着sqs班的课件。Slide上面写的代码没什么章法的贴出来，有些地方甚至sqs还批注：”此代码有误“。又找不到参考代码，也不知从哪里做起，真是折磨。好在其他课程还算进展顺利。</p>
<p>  四月印象当中还是很忙，不过如今回忆起来好像也忙的浑浑噩噩，清明节也没有时间出去玩。算是自己效率低下了吧。</p>
<p>  期中考试都还不错，除了ADS白给了两道，不过影响不大。</p>
<h2 id="科研"><a class="header-anchor" href="#科研">¶</a>科研</h2>
<p>  科研也算正式开始了。SRTP答辩之后申请到了省创，还是觉得可惜。杨洋老师给我分配的项目是关于AI制药的，我和我的小队选择了做分子的设计和优化的方向。答辩准备的很充足，项目也很好，难度也很大，本以为可以申请到国创。不过好在除了名号和经费的区别，实际上并没有什么太大的区别。</p>
<p>  虽然已经下定决心希望能通过这次项目发一篇paper了，但是四月的懈怠导致项目迟迟没有开始push。YY问我项目开始做没有，自己也有点不好意思，搪塞了几句。必须要开始花大功夫开始做科研了，毕竟如果能搞出不错的结果，对自己意义非凡。同专业的人都太强了，我现在还是远远不够。</p>
<p>  从长远来看，身边人的水平的确是在逐步提高，所以侧面说明自己在不断进步？小学初中拿第一较为简单，高中单科倒是拿过挺多次第一，综合只拿过一次。到了大学拿第一就很难了，同专业周围人都很刻苦，也很聪明，自己平常又爱偷懒，自然拉开了较大的差距。革命尚未成功，同志仍需努力。</p>
<p>  暑假多半是不会回去了。本来估摸着是要和好朋友们去哪玩玩，如今看起来确实难。把这个暑假花在科研上面对我来说能提升蛮多的。</p>
<p>  少说多做，少说多做。</p>
<h2 id="其他"><a class="header-anchor" href="#其他">¶</a>其他</h2>
<p>  实习结束了，互娱这边内推了90多个，雷火那边60多个，数据已经非常不错了，应该在所有大使里面能排进前10？互娱的基本工资在5月中旬发放了，还算不错。打算拿这些钱买个拍立得，再给外婆和婆婆买件衣服，剩下的余钱就先收着。</p>
<p>  四月除了课程实习科研，偶尔还是有空闲的时间。平常在看创造营，和《灵笼》、《窥探》。庆怜没有出道是没想到的。虽然时隔超男超女这么多年看选秀，从微博的舆论氛围和每次的排名还是能闻到现在饭圈和资本的异味，真的很畸形。这样的培养体系是真的“出道即巅峰”，因为大部分人的巅峰就在出道的那一晚上了hh。其他没有出道的倒像是被资本吸干血就扔掉了。饭圈也是混乱，每次排名一出真是腥风血雨。已经很难理解现在饭圈女孩的心理。看到青你魏宏宇和余景天的一些粉丝乱象，真的感到恶心。国家整治饭圈的确不无道理。</p>
<p>  《灵笼》真是国漫之光，各种意义上的。希望灵笼能走向世界，这是会让他国文化的人也会深受感触的作品。</p>
<p>  Rng时隔三年重夺春冠，老粉看到Gala五杀真的热泪盈眶（夸张，并没有）。希望在msi有好的成绩，最好夺冠，重振Rng荣耀。</p>
<h2 id="总结"><a class="header-anchor" href="#总结">¶</a>总结</h2>
<p>  四月算是平庸的一个月，不过过的也算比较舒服。总结来说，虽然时间基本都拿来work了，不过因为懈怠导致时间利用率很低。希望五月可以抓紧时间，这样多抽点时间来搞科研，偶尔也可以写写一些有意思的文章。从五月开始，争取更新一些有意思的文章和技术博客到网站上。</p>
<p>  五月加油~</p>]]></content>
      <categories>
        <category>Life</category>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
      </tags>
  </entry>
  <entry>
    <title>04\07 生活随笔</title>
    <url>/2021/04/07/04-07-%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="爷的心，禁止访问.jpg" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="9ca49fa81a2eeb3e6fb809025f474a8f345f0b3cef835549bb454a5c5c1c4e63">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">爱因斯坦说他猜出来了</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Life</category>
        <category>Essay</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
        <tag>Classified</tag>
      </tags>
  </entry>
  <entry>
    <title>一口会说话的锅</title>
    <url>/2021/03/28/%E4%B8%80%E5%8F%A3%E4%BC%9A%E8%AF%B4%E8%AF%9D%E7%9A%84%E9%94%85/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>  从前有一口锅。</p>
<p>  锅不是普通的锅，是城里最棒的铁匠，用最上乘的纯铁，花最长的时间打造出来的最好的锅。</p>
<p>  锅不是别人的锅，是铁匠的女儿小玉的锅。</p>
<p>  小玉用锅来做烙饼。每每小玉做烙饼的时候，香气总会沿着门缝溢出房门外，循着香气来到门前的人也就越来越多。小玉就笑着把门打开，把做好的烙饼分给大家。等到她自己能吃上的时候，不知道做了多少轮烙饼了。</p>
<p>  锅很幸福。锅不想做别的，只想静静地为小玉烙好一个又一个的饼子。</p>
<p>  小玉正值十七，是出嫁的妙龄。有一天，一个公子带了一大帮人来，带走了小玉。房子里只留下了捂着脸流泪的铁匠，几个还未煎好的烙饼，和孤零零的锅。锅突然开口说话了：</p>
<p>  ”小玉——“</p>
<p>  铁匠吓了一跳。锅从灶上蹦了下来，开始扑通扑通，一蹦一蹦朝着门外跳去。</p>
<p>  ”我要找到小玉！“</p>
<p>  从此，锅离开了铁匠的家，踏上了寻找小玉的征程。</p>
<br>
<p>  锅坚定无畏地向前行进着。不知道过了多久，走过了多少地方，但始终没有看到小玉的身影。</p>
<p>  锅在路上遇上了一个文官。文官把他拿起来打量了一番：“多好的锅！应该拿来熬树皮做纸。”</p>
<p>  锅讨厌做纸。锅挣脱开了文官的手，吭哧落了地。</p>
<p>  ”我要找到小玉，我要找到小玉。。。“</p>
<br>
<p>  锅坚定无畏地向前行进着。不知道过了多久，走过了多少地方，但始终没有看到小玉的身影。</p>
<p>  锅离开陆地，漂流在海上，遇到了一艘船。一个戴着帽子，棕色头发的航海家把他捞了起来：“多好的锅！应该拿来做菜，这样我就能撑住，直到找到新大陆了。”</p>
<p>  锅讨厌做菜。锅挣脱开了航海家的手，扑通跳下海。</p>
<p>  ”我要找到小玉，我要找到小玉。。。“</p>
<br>
<p>  锅坚定无畏地向前行进着。不知道过了多久，走过了多少地方，但始终没有看到小玉的身影。</p>
<p>  锅遇上了一支军队，被一个骑在马上的矮矮的将军抓了去。”多好的锅！这是我的战利品。“将军傲慢的说。</p>
<p>  锅讨厌被当作战利品。锅生气了，身体开始变得又红又热，将军被烫的松了手。锅蹦到了地上，气冲冲地离开了。</p>
<p>  ”我要找到小玉，我要找到小玉。。。“</p>
<br>
<p>  锅有一次遇上了一个少女。</p>
<p>  ”我要找到小玉，我要找到小玉。。。“</p>
<p>  锅没空搭理少女，继续往前行进着。</p>
<p>  少女感到自己被忽视了。她向锅喊道：</p>
<p>  ”喂！我就是小玉！“</p>
<p>  锅停了下来，回过头来看向少女。</p>
<p>  “小玉有一双像井水一样的眼睛。”</p>
<p>  少女眨了眨自己的大眼睛。</p>
<p>  “小玉有一头像瀑布一样的黑发。”</p>
<p>  少女取下了自己的发簪。</p>
<p>  “小玉有一颗像月亮一样的纯洁的心。”</p>
<p>  少女轻蔑地笑了笑。</p>
<p>  “你不是小玉。你什么也没有。”</p>
<p>  锅讨厌虚伪的少女。他摇了摇头，头也不回地离开了。</p>
<br>
<p>  “小玉就在世上的某一处。只要我挨个挨个找，一定能找到的。。。”</p>
<br>
<p>  锅遇见过草原上的长颈鹿，冰川中的北极熊，雨林里的猴子，海岸边的海鸥。无论刮风下雨、冰雹打雷，锅始终坚定无畏地寻找着小玉。</p>
<p>  周边的房屋越变越高，路上多了很多会奔跑的铁块。锅开始感到有点累了，原来锅不再是小锅，是口老锅啦。</p>
<p>  ”小玉。。。“</p>
<p>  锅怕自己撑不到找到小玉的那一天了。锅底开始泛蓝，是忧郁的蓝。</p>
<p>  锅在伤心。</p>
<br>
<p>  锅突然闻到了一股熟悉的香气。他一蹦一蹦，循着香味拐进了一个小巷，找到了一个破烂的小屋。小屋的窗前聚了很多人。锅在人群的缝隙间钻来钻去，钻到了人群的最前面。</p>
<p>  原来是一个老妇在屋子里做烙饼。老人的眼角挤着皱纹，头发花白，安静地用一口烂锅做着烙饼。</p>
<p>  ”小玉！小玉！“锅惊喜地跳了起来。屋前的人群被这口会说话的锅吓的赶紧逃走。</p>
<p>  锅用头咚咚的撞着屋门，呼唤着小玉的名字。但是老人仍然静静地做着烙饼。</p>
<p>  原来老人是个聋子。</p>
<p>  锅在门外等了一宿。第二天清晨，等老人打开屋门，才看见了锅。</p>
<br>
<p>  老人开始用锅来代替原来的烂锅做饼。每每老人做烙饼的时候，香气总会沿着窗缝溢出窗外，循着香气来到屋前的人也就越来越多。老人就笑着把窗户打开，把做好的烙饼分给大家。等到她自己能吃上的时候，不知道做了多少轮烙饼了。</p>
<p>  锅很幸福。锅不想做别的，只想静静地为老人烙好一个又一个的饼子。</p>
<p>  老人平常出门捡破烂卖钱，卖的钱买面团回来做饼。老人累了，会坐在锅旁边的椅子上休息。锅就开始滔滔不绝地讲他一路上有趣的经历：讲那个做纸的文官，讲那个傲慢的将军；讲草原与长颈鹿，讲海岸与海鸥。。。阳光照进小屋，老人的眼睛闭起来，规律地呼吸着。</p>
<p>  日复一日，故事讲完了就又从头开始讲起，锅乐此不疲。</p>
<p>  有一天，老人累了，靠在椅子上休息。眼睛闭了起来，却再也没有睁开。小屋门口渐渐聚集了越来越多的人，他们带走了老人。锅感到有点不安。过了几天，有人敲开了屋门，放了一个小木盒子在桌上。于是屋子里只剩下一个小木盒子，几个还未煎好的烙饼，和孤零零的锅。</p>
<p>  锅慢慢意识到了什么。他的身体开始变蓝，蓝的越来越深邃。</p>
<p>  锅变得很冷，比他去过的冰川还要冷。</p>
<br>
<p>  夜里，锅突然开始越来越烫，烧的通红，照亮了黑暗的小屋。天逐渐亮起来，锅的身体却慢慢变暗，成了墨一样的黑。原本还未煎好的烙饼重新滋滋作响，香气溢出晨光透射的窗缝。</p>
<p>  ”够啦。“</p>
<p>  从此，锅再也没有开口说过话。</p>]]></content>
      <categories>
        <category>Literature</category>
        <category>Tale</category>
      </categories>
      <tags>
        <tag>Fairy Tale</tag>
        <tag>Imaginative</tag>
      </tags>
  </entry>
  <entry>
    <title>你好，李焕英</title>
    <url>/2021/03/19/%E4%BD%A0%E5%A5%BD%EF%BC%8C%E6%9D%8E%E7%84%95%E8%8B%B1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div id="aplayer-IyqHaYDc" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="1822114811" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"></div>
<br>
<br>
<p>这是第一次在电影院里流泪，还不是轻轻地，是有点不能自已。以前老是被吐槽“铁心”。上次让我觉得备受触动的电影是高二时看的COCO，但也只是鼻头一酸。</p>
<br>
<p>这部电影打动我的第一个地方在于，她很真诚。当然，市面上很多电影都可以说自己很“真诚”，但我觉得他们往往只做到了真诚的“诚”，但是不够“真”。光有诚意是不够的。虽然说艺术源于生活而高于生活，但是能够引起观众共鸣的部分一定是和生活强关联的，同时出自于“真心”的。这部电影根据贾玲的母亲改编，我觉得她就很好地把自己想要表达的东西传递给观众了。没有什么高超的拍摄技巧，也许第一次执导还有些青涩，但就是通过简单的情节讲述了一个并不复杂的故事，然后打动了观众。看到电影最后贾玲带着她妈开敞篷跑车兜风，然而镜头一转，车上实际上只有她一个人，这时候我相信每位观众都能明白她内心真实的感受。</p>
<p>第二个就是，电影细节很好，并且很容易引起观众的共鸣。一部定位是“感人”的电影，你不能引起足够的共鸣，那么无论做什么都是没用的，顶多是主创团队自己感动自己，然后让观众知道“嗯，这里[应该]是感人的”，但其实观众并没有被感动到。对我来说，《送你一朵小红花》就是一个类似的例子。但是《你好，李焕英》里面每个细节都是非常容易打动人的，比如说我被打动的一些点：</p>
<blockquote>
<p>台词细节：</p>
<p>“我宝！”</p>
</blockquote>
<p>这句话是贾玲从天上飞下来，李焕英看到她脱口而出的一句话，然后说的同时冲过去抱住了贾玲。这句话只有两个字是真的好。“我宝”这两个字首先是非常地生活化，你换成“乖女儿！”“玲儿！”“危险！”等等任何其他词都会显得很蠢。这里就是要一个很顺口、很生活化的词，才能达到那种效果。并且说这个词的同时，张小斐双手呈张开状，脸上写满了担心，再配上这句话，真的是一下子就戳中人心。像这种类似的台词还有挺多：</p>
<blockquote>
<p>“你干什么来了？”</p>
<p>“让你开心来了！”</p>
<p>“真的呀！”</p>
<p>“对啊，你开心吗(* ^ _ ^ *)”</p>
<p>“我开心呐！”</p>
<p>“是吗？我还能让你更开心！”</p>
</blockquote>
<p>（台词记不太住了大概是这个）这段话其实也是挺朴实无华的，但是配合上演员的演绎，就简单的很触动人心，特别是当观众知道，其实说这句话的时候，李焕英也是穿越过来了的，这段简单的话就更值得回味了。细节十足。</p>
<p>除开台词，动作或者场景的细节也是蛮多的。</p>
<blockquote>
<p>“妈，我考上省艺校了！”</p>
</blockquote>
<p>当贾玲说这句话的时候，李焕英（老态）手里拿的面团，她听罢兴奋的拍手，面团上的面粉一下子在空中飞舞。并且这个时候阳光刚好从斜方照在李焕英的脸上，皱纹和飞起来的粉灰都泛着光。这一小段真的是特别好，一下子就把母女俩生活中的小美好表现了出来，非常贴近真实。而且这一小段放在结尾高潮煽情处，是经典的以乐景衬哀情，观众看到这一幕很难不被打动。还有一个类似的场景是李焕英在大雪天送贾玲离开，嘴上答应好她坐车回去，实际上为了省钱又不让贾玲担心，等贾玲离开后就自己一个人从车上下来，顶着风雪，走路回去了。这些动作、台词或者场景都很简单，但就是有着击中人心的力量。</p>
<p>再然后就是情节的设计。</p>
<p>情节的设计简直可以和《唐人街探案3》作正反面教材。唐探3一会儿又是寻找凶手，一会儿又是长泽雅美被绑架，剧情就变成了完成K给的任务，再然后和K进行了一堆狗屁不通的对话就去法院说找到凶手了。逻辑并不连贯。但是《你好，李焕英》的情节很清晰。穿越前就说了关于厂里的第一台电视和排球的事，然后穿越过去先是赶上了买电视，大家看完电视后厂长趁着高兴说起了排球比赛的事，这时候贾玲想起穿越前排球冠军有“好事”，就积极和李焕英准备比赛；准备完比赛发现好事是和沈光林相遇，贾玲觉得自己是累赘所以想让李焕英和沈光林在一起… 总之，情节是连贯的，并且转折都非常丝滑。并且全篇其实也就主要讲了两件事，一个是排球比赛，一个是撮合沈光林和她妈，但电影就把这两件事讲得很好，也就足够了。</p>
<p>情节不光简单，也很“合适”。印象深的是，开头贾玲作假被发现，全场瞬间安静的那段。按照正常的发展，这里应该会产生冲突，贾玲被李焕英痛斥一顿，再穿越过去悔悟。比如说《夏洛特烦恼》开头就是这样的展开。夏洛在秋雅婚礼上充大被马冬梅戳穿，产生“矛盾”，然后夏洛回到过去，才逐渐了解到马冬梅的好。但贾玲并没有选择这么设计。假证被发现后，画面一黑一转，李焕英载着贾玲回家。这里真的远超乎我意料。真的非常不喜欢假证被发现之后“理应”继续的情节，因为那个场面会很尴尬，让人看了也非常尴尬。这段让我的观感和好感一下子就提起来了。并且，李焕英载贾玲回去也并没有责备她，反而笑着“吐槽”女儿，也很真实的说“不用赚那么钱”。在贾玲发现妈妈没有生气之后，开始在自行车后座吹嘘自己的未来，李焕英只是笑着，和女儿一起说着美好的将来。这里也“一下子”就把李焕英的人物形象刻画出来了，不按常理出牌的设计刻画出了李焕英温柔慈祥的形象。</p>
<p>当然，还有最后“原来李焕英也穿越到了过去”的设计，真的是传神之笔，也让人有了二刷的意愿。没什么好说的，就是单纯的太厉害了。</p>
<p>打动我的最后一个点，就是人物本身的刻画。李焕英，真的是一位非常伟大的母亲，并且电影非常生动的描画出了这个形象。我想，这部电影之所以一定要贾玲来执导的原因，是因为只有她才知道，她的母亲在怎样的情形下，到底会说什么样的话，做什么样的动作，有什么样的神态。其余配角的刻画也很不错。</p>
<br>
<p>《你好，李焕英》的英文名是”Hi, Mom“。记者问贾玲，为什么中文名不也叫”你好，妈妈“。贾玲的回答是”妈妈首先是她自己，再是我的妈妈。“电影一开头，贾玲说她的印象中母亲一直是苍老的，但穿越过去，才看见了年轻漂亮，朝气活泼的李焕英。</p>
<p>因此我想，电影除了亲情的感动之外，也想告诉我们一个深刻的含义：任何母亲都不仅仅是”母亲“，首先是她”自己“，也曾拥有无忧无虑的童年，光彩夺目的青春。所以母亲除了作为“母亲”应该被儿女所爱，也应该作为一个独立的女性，被我们去尊重，去理解， 去爱。</p>]]></content>
      <categories>
        <category>Comments</category>
        <category>Film</category>
      </categories>
      <tags>
        <tag>Film</tag>
        <tag>Review</tag>
        <tag>Recommend</tag>
      </tags>
  </entry>
  <entry>
    <title>近期电影汇评</title>
    <url>/2021/03/18/%E8%BF%91%E6%9C%9F%E7%94%B5%E5%BD%B1%E6%B1%87%E8%AF%84/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="送你一朵小红花"><a class="header-anchor" href="#送你一朵小红花">¶</a>送你一朵小红花</h2>
<p>一部70分的电影。70分主要是给配角们的演技，以及拍摄的质量，还有电影的主题。</p>
<p>剩下30分主要是槽点太多了。</p>
<p>最大的槽点是男女主雨中相拥那一段。在电影当中，这里前面一开始两人在争论，而且不是普通的争吵，是一个很严重的社会话题，我作为观众的情绪也应该是往思考和沉重方面靠。结果男住突然来一句”我喜欢你“，然后两个人雨中相拥。现代医患心理深刻问题矛盾一秒变少年雨中青涩爱情桥段。</p>
<p>这段前面两人的争吵也很神奇。女主说服男主的方式居然是随便在门口找了一个寻亲的奶奶和境遇悲惨的送外卖的来比惨。这两个角色前面一点铺垫没有，突然冒出来，这不是铁工具人这是什么。而且大雨天这俩工具人也站在门口，在没有铺垫的情况下会显得不合理和突兀。</p>
<p>不合适的情节还有很多。比如男主匿名给失去女儿的父亲点红烧牛肉盖饭。这个地方解读很容易产生差异和分歧，有一些人会把这里解读为陌生人的温暖，但也会有部分人认为这里是迷惑操作。</p>
<p>再说说主题。</p>
<p>一部两个小时的电影我看了感觉看了得有3个小时。这部电影想表达的主题太多了：医患心理，亲情，爱情，患者之间的帮扶友情… 当然，在患者的生活中，这些元素都是有的。但是电影没有安排一个脉络清晰的逻辑主线，让这些元素显得非常乱，并且也没有一个明显的主次关系。在观影过程中，大概就是让人看到某个情节，然后知道“这里是个泪点”，但也就仅仅知道而已，其实内心毫无波动。同时，密集而没有突出的泪点也容易让观众产生疲劳。全片看下来我唯一觉得做的不错的就是韦一航家庭，父母和儿子关系这一part还可以。</p>
<p>《送你一朵小红花》的想法是好的，诚意也是有的，但是覆盖了很多华而不实的表皮，演员演技也参差不齐，让人很难入戏。</p>
<h2 id="心灵之旅"><a class="header-anchor" href="#心灵之旅">¶</a>心灵之旅</h2>
<p>皮克斯真是从来没让我失望。</p>
<p>一直觉得皮克斯做的动画是全龄向的童话，这一部更是证明了这一点。动画虽然看起来简单且儿龄向，设定与情节也算比较老套的了，但是即使是成年人看了也会深受感动，因为皮克斯精准而又得当的表达了一个深刻的主题。之前的COCO也是一样。</p>
<p>成人童话真的是非常必要的。实际上每个大人都有属于自己的幻想世界，也许甚至比小朋友的更丰富多彩，但是却越来越隐秘了。现实生活中，人们随着年龄的增长在心灵处处设防，所以很多描述实际的文学影视作品难以感动成年人群。但是童话是一座完全不同于现实作品的桥梁，他能绕开层层的厚墙，直通每个人心中尚在的一片理想花园，轻吟一段神秘的咒语，便能浇灌人们心中干枯的花蕊。</p>
<p>《小王子》就是这样的一部作品，皮克斯的动画也是。</p>
<h2 id="刺杀小说家"><a class="header-anchor" href="#刺杀小说家">¶</a>刺杀小说家</h2>
<p>感觉结尾还是有点可惜，有点像一副好牌打得稀烂。不知道是改编的问题还是原作本来结尾就很烂。</p>
<p>整部作品的设定是非常有有意思的，现实与小说的映射，执笔主宰未来的人生。电影部分情节的转场与特效也很漂亮，印象深刻的有两个：一个是路空文在顶楼差点摔落的时候，眼睛看往楼下而转到了另一个世界。这一段两个世界的无缝衔接很棒。还有一个是关宁和小橘子被红衣武士看见，准备追杀时，魁梧的红衣武士背靠夕阳，身披长铠，手执大刀。这个画面让人印象深刻，可能是画面构图以及红衣武士设计的原因。</p>
<p>但是整体主题并不突出，想做反乌托邦但也没能很好地体现，结尾也是有点哭笑不得，感觉有点可惜。不过还是比《唐人街探案》好吧。</p>]]></content>
      <categories>
        <category>Comments</category>
        <category>Film</category>
      </categories>
      <tags>
        <tag>Film</tag>
        <tag>Review</tag>
      </tags>
  </entry>
  <entry>
    <title>三月上</title>
    <url>/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="294" height="86" src="//music.163.com/outchain/player?type=2&id=1472921626&auto=1&height=66">
</iframe>
<h2 id="日常"><a class="header-anchor" href="#日常">¶</a>日常</h2>
<p>深刻意识到大一和大二上欠“债”太多，现在开始要好好调整安排和作息。</p>
<p>买了一个驼色的秋刀鱼TN手账，旅行款的那种，比较小一个，适合随身携带。看了下网上的手账使用方法，发现和自己想象的不是同一个世界。原来做手账也是一门学问，好的手账er可以做的非常精美有意思。不过对于我这种工科男来讲，能用的就是记录一些idea和要做的事情、问题之类的。或者贴一些有意思的东西，比如去年去迪士尼留下来的门票。</p>
<p>每天早上7：20起床，8：00到图书馆进朗读亭开始读英语。上大学之前以为自己的英语口语还是不错的，直到进入广播站英文专题才发现，简直就是xx。开始听VOA跟读，打开电脑看B站上面Lisa的美音练习，练到9：00，再自习一会儿就去上课。晚上十点后开始自学日语，学到十一点上床睡觉。</p>
<p>这学期玩的时间几乎没有了，手机上游戏和B站都卸了，只有偶尔看看Rng的比赛，或者看点别的。最近在看创造营2021，觉得还挺有意思的，现在基本每一期都会看。打算近期写一篇相关文章评价这次创造营2021。</p>
<h2 id="实习"><a class="header-anchor" href="#实习">¶</a>实习</h2>
<p>寒假的时候面试了4次，一次是网易雷火的游戏研发岗。这个面试卡了，一是因为暑期实习时间不够，二是岗位要求比我想象的更多，主要面向大三及以上的学生，和我这种小屁孩儿没什么关系。</p>
<p>第二次是字节跳动的校园大使。面试官上来就是问我运营了什么什么社群，有没有什么组织经历，是学生会的人吗？本着良心起见，我一律摇了摇头。再一问我发现我是大二的，她开始说：“嗯嗯，好的好的，大概情况我了解了。”我心里也明白，她的意思是：”嗯嗯，你可以走了“。</p>
<p>后两次分别是网易雷火和互娱的校园大使面试，经过前一次的惨败我有教训了，开始了<s>不要脸</s>适度的夸大自己。</p>
<blockquote>
<p>在广播台里任职，传媒资源广               -&gt; 其实只是每周做节目而已，跟传媒一点关系都没有。</p>
</blockquote>
<blockquote>
<p>虽然是大二，但认识非常多的学长       -&gt; 认识两个学长</p>
</blockquote>
<blockquote>
<p>和大二大三的年级辅导员关系都好       -&gt; 和两个辅导员各交流过两句话</p>
</blockquote>
<blockquote>
<p>经常在各大bbs混迹                                -&gt; 在bbs上发过两条卖书帖</p>
</blockquote>
<blockquote>
<p>我将如何宣传网易：xxxxxxxxx            -&gt; 吹牛有一手的</p>
</blockquote>
<blockquote>
<p>交友范围广，涉及各大学院                   -&gt; 呵呵</p>
</blockquote>
<p>总而言之，凭借着自己的真才实学得到了两个校园大使的Offer，后来发现不冲突，正好又都是网易的时候，就干脆一起当了。进去之后才发现，无论是网易还是互娱，我都是唯一一个大二的…</p>
<p>感觉互娱和雷火好不一样。都是网易游戏旗下事业部，雷火一进去就是HR让你宣传，拿着内推码去拉人，其他什么也没说，文案什么的都得自己想自己编，怎么宣传也是自己的事，所以刚进去一脸懵逼。</p>
<p>互娱刚进去的时候还搞了一个校园大使欢迎会，不同地区（华东，华南…）的组队，做自我介绍，还有xx活动，还要表现评级，第一名的队伍还有网易周边… 搞得还蛮隆重的。我们那一组就是江浙一代的，都是东南、南大、南邮、南理、浙大。所以给队伍取名字的时候，我想到用地域谐音梗取名，取了一个：</p>
<blockquote>
<p>南道旧浙（难道就这）</p>
</blockquote>
<p>HR看了直呼鬼才，hh。</p>
<p>雷火每天的校园大使群里HR只负责答疑，互娱校园大使群里HR小姐姐感觉和大家都很亲近，并且每天都有新的任务发布，而且有专门的worktile工作系统。怎么拉社群，怎么编辑文案，怎么和大家沟通，互娱HR每天都会有新的相关物料放送。难道就这队里有几位老前辈也会分享相关经验，我还是挺收益匪浅的。</p>
<p>实习的第一个难点就是拉社群。雷火要求社群至少100个，互娱至少200个。对我来说真的是太难了。面向大二我都还好，关键是岗位基本都是面向大三的，我能碰到的大三学生资源太少了。无奈，只有在学校BBS里发帖，然后每天顶好几次帖，保持高曝光率。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/bbs1.png" alt="bbs1"></p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/bbs2.png" alt="bbs2"></p>
<p>然后再让大学好友高中兄弟们去帮忙转发拉人啦，找辅导员或者学长帮忙转发在大三的社群里啦… 也算是凑齐了人数。</p>
<p>第二个难点就是时刻被打扰。现在还在学习，但是一天到晚就会有很多不同的人来私戳问你各式各样的问题。有意思的是，基本都以为我是网易员工，以为是前辈，无论是大三大四还是硕士，加了我好友第一句都是：</p>
<p>”师哥好！“</p>
<p>一开始我还解释，后来就懒得解释了。问题还是蛮多样的，有的人真的是时时刻刻都充满了忧虑，问一些我都不知道咋回答的问题”我的简历怎么还没筛过啊？“”你这个内推真的有用吗？“”你说我该不该投递这个岗位啊？“”面试官问我xxxx我该怎么办“”我有一个条件不符合怎么办…“</p>
<p>还有的人跟你聊了几句就开始聊人生，讲他最近咋咋咋，什么不顺心啦，然后开始讲起他的过去… 你要耐心的听完，并给出一些合理的建议。</p>
<p>开始的两周真的是累死。想文案，写文案，每天运营社群，联系各种资源…</p>
<p>好在势头还不错，全国90多个校园大使排到了第7，互娱HR奖励了一个阴阳师达摩杯。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/%E5%86%85%E6%8E%A8.jpg" alt="内推"></p>
<p>作为一个大二生，还是蛮不容易的hh。不过之后的内推的人越来越少了，能不能在4月中旬（工作结束）前完成指定kpi，还是个未知数。</p>
<h2 id="社会实践"><a class="header-anchor" href="#社会实践">¶</a>社会实践</h2>
<p>寒假的时候搞了红色社会实践，一个25个人的团，我是团长。一开始本来想去抱大腿去加别人的团，结果别人的腿都被抱完了，只好伸出了自己瘦瘦的脚杆。结果发现和我一样的人还有很多，于是上了我的贼船。后来这个船越来越大，什么药学院传媒学院外国语学院的都来了，就成了个25个人的团。当时立项申请书还有填申报表基本都是我一个人写的，搞了好久，还蛮累得。</p>
<p>这学期开始把材料啥的都交了，以为就结束了。结果辅导员找到我说，你们这个团搞得不错啊，学院里打算推一个名额去参加挑战杯今年临时开的红色专项，就推你们了！当时是周三晚跟我说的，结果下周一就要交了。比赛要交一个5min短视频和万字调研报告，本来打算就用寒假社会实践做的视频随便挑一个上去，调研报告就从自己写的总结报告里改改，没想到辅导员甩给我一个别的学校做的视频链接，点进去一看，好家伙，怎么做的这么好。下来一查才发现，这挑战杯红色专项老早在寒假就发布了，别的学校很早就开始准备，结果我们学校截止前一周不到发布临时通知，让每个学院推一个名额上去。我一下子懂了我们团是干什么的了，这波是临时被抓上阵的炮灰。</p>
<p>辅导员还和我语重心长的说：”你们团是咱计院的独苗苗啊！加油，我看好你们。“</p>
<br>
<p>学院只让我带9个人去参加比赛。要从24个人当中选9个，我只好客观的选了寒假社会实践参与度高的同学进来，这样大家都尽量服气。另外保了一下寒假做视频的同学，这样好准备这次的短视频。</p>
<p>接下来就是紧急召集团队，策划视频，写文案，找人借摄像机拍一些补充视频素材，最后剪辑加特效、渲染。</p>
<p>准备比赛这几天真的很忙，我感觉我上学期期末都没这么忙。每天凌晨一点睡，早上7点起，组织拍视频，和技术组的同学沟通，还有监督文案的配音。因为我负责了主要的策划，所以每个人在做的时候我都要在旁边不断地交流想法。加上我只让了两个视频剪辑的同学进组，这次剪辑和渲染的时间也特别紧张，还得临时调整方案。</p>
<p>结果到了最后一天，做视频开头的同学的电脑由于配置太低，渲染的时候死机了，没时间做结尾了。然后正好当时又联系不上他，半天不知道他那边是啥情况，发了十条信息，可能隔了三个小时回了我一条不知所云的信息，然后就又断开链接了。当时我还在咖啡店写具体的分镜和调研报告的结尾，一边写一边着急，因为第二天中午就要交稿了。后来实在没办法，向上面请求宽限了一天，动用了钞能力，找淘宝上一家视频代做的做一下结尾，打电话和那个淘宝剪辑师交流，再把我写的分镜给他，最后也还算完成了。</p>
<p>3月16日，材料上交的第二天早上没课，我好好地补了一觉，十点才起的床。</p>
<br>
<p>这次比赛虽然结果还没出来，但我已经挺有收获的。现在都还记得交稿前一天晚上在咖啡店联系不上剪辑的同学而焦急的自己。明明已经很努力了，有一天午饭没来得及吃，有一天晚饭没吃，但最后还是那么狼狈。一直在想，到底是组员的问题，还是我组长的问题，还是说学院这么晚发通知的问题？非常怀疑自己有没有做组长的能力。</p>
<p>后来反思了一下，觉得主要还是自己的问题。当时应该多招一点技术剪辑的同学进来，而且我想做的很好，导致视频剪辑难度变高，很难在这么几天赶出来，无法跟那些从寒假就开始准备的队伍比较。自己决策失误了。人员的调度也有点问题。最后没联系上剪辑同学的时候也不该很焦急，应该冷静分析一下该怎么办，比如先把别的处理好或者联系别的同学寻求帮助，方法还是很多的。</p>
<h2 id="科研"><a class="header-anchor" href="#科研">¶</a>科研</h2>
<p>寒假的时候开始有搞科研的想法，于是于三月初一个星期四的晚上，写好了邮件发给了我心仪的导师：杨洋。</p>
<p>杨洋是我的班主任，不过只有大二上开学见过一次，后面就没见过了，直到这学期他是我的ADS老师。大二上第一次见面的时候以为他是我的同学，只是长得比较成熟，没想到居然是我们的班主任，真是不可思议。身高挺高的，穿着很简单，喜欢笑，笑起来是比较憨厚的那种。但是他说几句俏皮话之后，就会明白这个人应该是属于聪明那一挂的。</p>
<p>后来才知道，他是清华博士，现在是浙大的博导+副教授，人工智能系主任，才30出头，真是年少有为啊。不仅如此，居然都有两个孩子了，真的是把我震撼到了。毕竟第一次见面的时候我还以为他是我同学。</p>
<p>他人感觉很风趣，也很可靠，并且也非常有实力，上ADS的时候感觉也很照顾学生。读了他的一些论文，再去了解他做的领域（dm），考虑再三之后，最终决定好了要发邮件。描述了一下自己过往的一些经历，和自己的一些想法。反复审查几遍没什么问题之后，邮件就发送出去了。</p>
<p>没想到第二天早上就有了回复。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/%E7%A7%91%E7%A0%94.png" alt="科研"></p>
<p>看到回复的时候当时应该是这学期目前最高兴的时候。</p>
<p>后来和同进组的同学交流了一下，发现我进的还是很快很顺利的。可能是他觉得我高中有竞赛基础吧。</p>
<p>加了他微信后，又加了实验室的群，科研之旅也算是正式开始了。已经做好了学一堆新知识看一堆论文的准备了。</p>
<p>PS：杨洋老师的头像和他本人一样的有趣。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/%E6%9D%A8%E6%B4%8B.jpg" alt="杨洋"></p>
<h1>三月上</h1>
<p>とにかく，三月上还是很忙的。忙到没有时间也没有心情去看学校里到处开的花儿。广播站有同学趁着周末去了趟武汉赏花，想来还是挺羡慕的。</p>
<p>不过换种方式想想，我也算忙的充实。迈入了最期待的科研大门，希望能开启一段有意思的旅程。</p>
<p>GoGoGo 😎</p>]]></content>
      <categories>
        <category>Life</category>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/02/09/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>作为第一篇文章，希望这里能成为我喜欢休憩的净土，和一个有趣的世界。</p>]]></content>
      <tags>
        <tag>Beginning[开始]</tag>
      </tags>
  </entry>
</search>
